---
title: "Methods 2 -- Portfolio Assignment 1"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

- _Type:_ Group assignment
- _Due:_ 10 March 2024, 23:59

---

In the following exercises, you will be asked to generate and summarize simulations from statistical models. You should use what you have learned so far (i.e. for loop, if else statements, sampling from continuous and discrete distributions...) to generate observations and summarize your samples using (one of) the appropriate methods. You can find examples of how to do that in Ch. 5. Note that here we will only focus on generative models, several aspects for inference and hypothesis testing discussed in Ch. 4 are not directly needed in this context.

In the first exercise, we will assume that the population of interest has a proportion of 0.51 men and 0.49 women. Your model should reflect that.

Please submit your answers on GitHub Classroom.

Karoline, Regitze & Katrine

```{r loading packages}
library(pacman)
pacman::p_load(tidyverse,
               ggpubr,
               ggplot2,
               stringr,rstanarm)
```


---

1. _(5.2 from ROS)_ __Continuous probability simulation:__ The logarithms of weights (in pounds) of men in the United States are approximately normally distributed with mean 5.13 and standard deviation 0.17; womenâ€™s log weights are approximately normally distributed with mean 4.96 and standard deviation 0.20. Suppose 10 adults selected at random step on an elevator with a capacity of 1750 pounds. What is the probability that their total weight exceeds this limit?

This code is written by Karoline
```{r}
#setting seed for reproducibility
set.seed(123)

#number of trials in simulation
n_sims <- 1000

#creating empty vector
avg_total_weight <- rep(NA, n_sims)

#for-loop
for (s in 1:n_sims) {
  N <- 10 #10 adults per simulation
  genders <-
    rbinom(N, 1, 0.49) #defining a binomial distribution with female proportion 
  weights <-
    exp(ifelse(genders == 1, rnorm(N, 4.96, 0.20), rnorm(N, 5.13, 0.17))) # simulating weights in normal distributions and converting log of weights to weights in pounds
  avg_total_weight[s] <- sum(weights) #summing to get the total weight of the 10 people
}

#calculating probabiiity that total weight exceeds 1750 pounds
probability_exceeds <- sum(avg_total_weight > 1750) / n_sims
print(probability_exceeds)

#The probability that the total weight exceeds 1750 pounds is ~5%

```

2. _(5.6 from ROS)_ __Propagation of uncertainty:__ We use a highly idealized setting to illustrate the use of simulations in combining uncertainties. Suppose a company changes its technology for widget production, and a study estimates the cost savings at \$5 per unit, but with a standard error of \$4. Furthermore, a forecast estimates the size of the market (that is, the number of widgets that will be sold) at 40 000, with a standard error of 10 000. Assuming these two sources of uncertainty are independent, use simulation to estimate the total amount of money saved by the new product (that is, savings per unit, multiplied by size of the market).

This code is written by Regitze

```{r}
set.seed(123) # Setting seed for reproducibility

# Defining parameters
mean.savings <- 5
se.savings <- 4
size.market <- 40000
se.market <- 10000
n_sims2 <- 10000 # creating a number of 10000 simulations
total_savings <- rep(NA,n_sims2) # creating an empty vector


# making a loop 
for (i in 1:n_sims2){
  cost_savings <- rnorm(1,mean.savings,se.savings) # creating a random normal distribution of cost savings
  market_size <- rnorm(1,size.market,se.market) # random normal distribution of size of market
  total_savings[i] <- cost_savings*market_size # defining calculation for total savings
}

# mean of all total savings
mean(total_savings)

# The total amount of money saved is estimated at ~ $199451 

```

3. _(5.10 from ROS)_ __Inference for a ratio of parameters:__ A (hypothetical) study compares the costs and effectiveness of two different medical treatments.

    - In the first part of the study, the difference in costs between treatments A and B is estimated at \$600 per patient, with a standard error of \$400, based on a regression with 50 degrees of freedom.
  
    - In the second part of the study, the difference in effectiveness is estimated at 3.0 (on some relevant measure), with a standard error of 1.0, based on a regression with 100 degrees of freedom.
    - For simplicity, assume that the data from the two parts of the study were collected independently.

    Inference is desired for the incremental cost-effectiveness ratio: the difference between the average costs of the two treatments, divided by the difference between their average effectiveness, a problem discussed further by Heitjan, Moskowitz, and Whang (1999).

    (a) Create 1000 simulation draws of the cost difference and the effectiveness difference, and make a scatterplot of these draws.
    (b) Use simulation to come up with an estimate, 50% interval, and 95% interval for the incremental cost-effectiveness ratio.
    (c) Repeat, changing the standard error on the difference in effectiveness to 2.0.
    
This code is written by Katrine
```{r}
difcost <- 600
costerror <- 400
difeff <- 3
erroreff <- 1

# degrees of freedom
dfcost <- 50
dfeff <- 100

# assigning sample sizes 
n_cost <- dfcost+1
n_eff <- dfeff+1

# Set the seed for reproducibility
set.seed(123)

# Number of simulations
num_simulations <- 1000
cost_difference<- c()
effectiveness_difference<- c()

# Simulation draws for cost difference and effectiveness difference

for (i in 1:1000){
  cost_difference[i] <- mean(rnorm(n_cost,600,400))
  effectiveness_difference[i] <- mean(rnorm(n_eff, 3, 1.0))
}


df <- data.frame(cost_difference,effectiveness_difference) # creating dataframe
# Scatterplot
ggplot(df,aes(cost_difference,y=effectiveness_difference, color = "red")) +
  geom_point() +
  theme_bw() + 
  labs(title = "Scatterplot", x = "Cost Difference", y = "Effectiveness Difference")
  

# b) 

df$ratio <- df$cost_difference/df$effectiveness_difference

ggplot(df,(aes(x=ratio, col= "red"))) +
  geom_histogram(binwidth=1) +
  xlim(100,300)

#calculating mean, median and mad
mean(df$ratio)
median(df$ratio)
mad(df$ratio)

quantile(df$ratio, c(.25, .75))	
quantile(df$ratio, c(.025, .975))	

#Mean is 199.8, median is 199.7, mad is 18.8
#Our estimate is that 50% of data has a cost/effectiveness ratio between [186;212]
#Our estimate is that 95% of data has a cost/effectiveness ratio between [163;240]
```

This code is written by Karoline
```{r}
# c)
set.seed(123)
cost_difference2 <- c()
effectiveness_difference2 <- c()

for (s in 1:num_simulations) {
  cost_difference2[s] <- mean(rnorm(51, difcost, costerror))
  effectiveness_difference2[s] <- mean(rnorm(101, difeff, 2))
}

#adding a column in df with ratio
df2 <- data.frame(cost_difference2,effectiveness_difference2) # creating dataframe
df2$ratio <- df2$cost_difference2/df2$effectiveness_difference2

#finding quantiles
quantile(df2$ratio, c(.25, .75))	
quantile(df2$ratio, c(.025, .975))

#calculating mean, median and mad
mean(df2$ratio)
median(df2$ratio)
mad(df2$ratio)

#Mean is 200.3, median is 199.6, mad is 22.2
#Our estimate is that 50% of data has a cost/effectiveness ratio between [185;217]
#Our estimate is that 95% of data has a cost/effectiveness ratio between [158;249]

```


